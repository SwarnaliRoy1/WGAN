{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":120695,"databundleVersionId":14436413,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-16T17:18:02.564067Z","iopub.execute_input":"2025-11-16T17:18:02.564229Z","iopub.status.idle":"2025-11-16T17:18:05.314799Z","shell.execute_reply.started":"2025-11-16T17:18:02.564207Z","shell.execute_reply":"2025-11-16T17:18:05.314023Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/test-gan-competition/shard_02/data.jsonl\n/kaggle/input/test-gan-competition/shard_45/data.jsonl\n/kaggle/input/test-gan-competition/shard_09/data.jsonl\n/kaggle/input/test-gan-competition/shard_37/data.jsonl\n/kaggle/input/test-gan-competition/shard_35/data.jsonl\n/kaggle/input/test-gan-competition/shard_15/data.jsonl\n/kaggle/input/test-gan-competition/shard_31/data.jsonl\n/kaggle/input/test-gan-competition/shard_50/data.jsonl\n/kaggle/input/test-gan-competition/shard_20/data.jsonl\n/kaggle/input/test-gan-competition/shard_25/data.jsonl\n/kaggle/input/test-gan-competition/shard_46/data.jsonl\n/kaggle/input/test-gan-competition/shard_39/data.jsonl\n/kaggle/input/test-gan-competition/shard_40/data.jsonl\n/kaggle/input/test-gan-competition/shard_44/data.jsonl\n/kaggle/input/test-gan-competition/shard_33/data.jsonl\n/kaggle/input/test-gan-competition/shard_59/data.jsonl\n/kaggle/input/test-gan-competition/shard_53/data.jsonl\n/kaggle/input/test-gan-competition/shard_54/data.jsonl\n/kaggle/input/test-gan-competition/shard_42/data.jsonl\n/kaggle/input/test-gan-competition/shard_41/data.jsonl\n/kaggle/input/test-gan-competition/shard_04/data.jsonl\n/kaggle/input/test-gan-competition/shard_07/data.jsonl\n/kaggle/input/test-gan-competition/shard_47/data.jsonl\n/kaggle/input/test-gan-competition/shard_18/data.jsonl\n/kaggle/input/test-gan-competition/shard_08/data.jsonl\n/kaggle/input/test-gan-competition/shard_03/data.jsonl\n/kaggle/input/test-gan-competition/shard_43/data.jsonl\n/kaggle/input/test-gan-competition/shard_55/data.jsonl\n/kaggle/input/test-gan-competition/shard_06/data.jsonl\n/kaggle/input/test-gan-competition/shard_12/data.jsonl\n/kaggle/input/test-gan-competition/shard_11/data.jsonl\n/kaggle/input/test-gan-competition/shard_51/data.jsonl\n/kaggle/input/test-gan-competition/shard_13/data.jsonl\n/kaggle/input/test-gan-competition/shard_38/data.jsonl\n/kaggle/input/test-gan-competition/shard_52/data.jsonl\n/kaggle/input/test-gan-competition/shard_17/data.jsonl\n/kaggle/input/test-gan-competition/shard_16/data.jsonl\n/kaggle/input/test-gan-competition/shard_26/data.jsonl\n/kaggle/input/test-gan-competition/shard_48/data.jsonl\n/kaggle/input/test-gan-competition/shard_00/data.jsonl\n/kaggle/input/test-gan-competition/shard_27/data.jsonl\n/kaggle/input/test-gan-competition/shard_24/data.jsonl\n/kaggle/input/test-gan-competition/shard_21/data.jsonl\n/kaggle/input/test-gan-competition/shard_57/data.jsonl\n/kaggle/input/test-gan-competition/shard_28/data.jsonl\n/kaggle/input/test-gan-competition/shard_32/data.jsonl\n/kaggle/input/test-gan-competition/shard_29/data.jsonl\n/kaggle/input/test-gan-competition/shard_10/data.jsonl\n/kaggle/input/test-gan-competition/shard_19/data.jsonl\n/kaggle/input/test-gan-competition/shard_49/data.jsonl\n/kaggle/input/test-gan-competition/shard_14/data.jsonl\n/kaggle/input/test-gan-competition/shard_23/data.jsonl\n/kaggle/input/test-gan-competition/shard_56/data.jsonl\n/kaggle/input/test-gan-competition/shard_30/data.jsonl\n/kaggle/input/test-gan-competition/shard_36/data.jsonl\n/kaggle/input/test-gan-competition/shard_05/data.jsonl\n/kaggle/input/test-gan-competition/shard_58/data.jsonl\n/kaggle/input/test-gan-competition/shard_01/data.jsonl\n/kaggle/input/test-gan-competition/shard_22/data.jsonl\n/kaggle/input/test-gan-competition/shard_34/data.jsonl\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## 1. Imports + config + decode function","metadata":{}},{"cell_type":"code","source":"import base64, io, json, os\nfrom glob import glob\n\nimport numpy as np\nfrom PIL import Image, ImageOps, ImageFile, UnidentifiedImageError\n\nImageFile.LOAD_TRUNCATED_IMAGES = True  # tolerate truncated images\n\n\ndef get_img_b64(rec):\n    \"\"\"\n    Simple, explicit key lookup for the main image.\n    \"\"\"\n    for k in [\"img_b64\", \"img64\", \"img_64\"]:\n        v = rec.get(k)\n        if isinstance(v, str) and v.strip():\n            return v.strip()\n    return None\n\n\ndef b64_to_bytes(s: str) -> bytes:\n    \"\"\"\n    Base64 decode with only padding fix.\n    \"\"\"\n    s = s.strip()\n    missing = (-len(s)) % 4\n    if missing:\n        s += \"=\" * missing\n    return base64.b64decode(s)\n\n\ndef decode_record_to_image(rec):\n    \"\"\"\n    Minimal, spec-following decoder:\n\n      1. Base64 -> bytes (from img_b64/img64/img_64)\n      2. Open with PIL\n      3. Apply EXIF rotation and inversion\n      4. Handle alpha (built-in RGBA) by compositing on white\n      5. Convert to RGB\n      6. Resize to 32x32\n\n    Returns a 32x32 RGB PIL.Image or None on failure.\n    \"\"\"\n    try:\n        # 1. Decode Base64 -> bytes\n        b64 = get_img_b64(rec)\n        if b64 is None:\n            return None\n\n        try:\n            img_bytes = b64_to_bytes(b64)\n        except Exception:\n            return None\n\n        # 2. Open using PIL\n        try:\n            bio = io.BytesIO(img_bytes)\n            img = Image.open(bio)\n            img.load()\n        except (UnidentifiedImageError, OSError):\n            return None\n\n        mode_field = rec.get(\"mode\", img.mode)\n        exif_rot = rec.get(\"exif_rot\", 0)\n        invert_flag = rec.get(\"invert\", False)\n\n        # 3. EXIF rotation\n        if exif_rot:\n            img = img.rotate(exif_rot, expand=True)\n\n        # Basic mode handling, including I;16\n        if mode_field == \"I;16\" or img.mode == \"I;16\":\n            # convert 16-bit array to 8-bit with a simple rescale\n            arr = np.array(img, dtype=np.uint16)\n            vmax = arr.max()\n            if vmax > 0:\n                arr8 = (arr.astype(np.float32) / vmax * 255.0).astype(np.uint8)\n            else:\n                arr8 = np.zeros_like(arr, dtype=np.uint8)\n            img = Image.fromarray(arr8, mode=\"L\")\n        elif mode_field == \"RGBA\":\n            img = img.convert(\"RGBA\")\n\n        # Inversion (on 8-bit)\n        if invert_flag:\n            img = img.convert(\"L\")\n            img = ImageOps.invert(img)\n\n        # 4. Handle alpha channels if present (built-in only)\n        if img.mode == \"RGBA\":\n            bg = Image.new(\"RGBA\", img.size, (255, 255, 255, 255))\n            img = Image.alpha_composite(bg, img)\n            img = img.convert(\"RGB\")\n\n        # 5. Convert to RGB\n        if img.mode == \"L\":\n            img = img.convert(\"RGB\")\n        elif img.mode != \"RGB\":\n            img = img.convert(\"RGB\")\n\n        # 6. Resize to 32×32\n        img = img.resize((32, 32), resample=Image.BICUBIC)\n\n        return img\n\n    except Exception:\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T17:18:11.936213Z","iopub.execute_input":"2025-11-16T17:18:11.936453Z","iopub.status.idle":"2025-11-16T17:18:11.960167Z","shell.execute_reply.started":"2025-11-16T17:18:11.936434Z","shell.execute_reply":"2025-11-16T17:18:11.959617Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## 2. Iterate over shards & save PNGs","metadata":{}},{"cell_type":"code","source":"INPUT_ROOT = \"/kaggle/input/test-gan-competition\"\nOUT_ROOT = \"/kaggle/working/train_images\"\nOUT_CLASS_DIR = os.path.join(OUT_ROOT, \"all\")  # ImageFolder expects subfolders\n\nos.makedirs(OUT_CLASS_DIR, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T17:18:15.570375Z","iopub.execute_input":"2025-11-16T17:18:15.570631Z","iopub.status.idle":"2025-11-16T17:18:15.574633Z","shell.execute_reply.started":"2025-11-16T17:18:15.570611Z","shell.execute_reply":"2025-11-16T17:18:15.574071Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import shutil\nfrom tqdm.auto import tqdm\n\nif os.path.exists(OUT_CLASS_DIR):\n    shutil.rmtree(OUT_CLASS_DIR)\nos.makedirs(OUT_CLASS_DIR, exist_ok=True)\nprint(\"OUT_CLASS_DIR reset:\", OUT_CLASS_DIR)\n\njsonl_paths = sorted(glob(os.path.join(\n    \"/kaggle/input/test-gan-competition\", \"shard_*\", \"data.jsonl\"\n)))\nprint(\"Found shards:\", len(jsonl_paths))\n\nnum_saved = 0\nnum_failed = 0\n\nfor jsonl_path in jsonl_paths:\n    print(\"Processing\", jsonl_path)\n    with open(jsonl_path, \"r\") as f:\n        for line in tqdm(f, mininterval=1.0, leave=False):\n            line = line.strip()\n            if not line:\n                continue\n            try:\n                rec = json.loads(line)\n            except json.JSONDecodeError:\n                num_failed += 1\n                continue\n\n            img = decode_record_to_image(rec)\n            if img is None:\n                num_failed += 1\n                continue\n\n            img_id = rec.get(\"id\")\n            if not img_id:\n                num_failed += 1\n                continue\n\n            out_path = os.path.join(OUT_CLASS_DIR, f\"{img_id}.png\")\n            if not os.path.exists(out_path):\n                img.save(out_path)\n                num_saved += 1\n\nprint(\"Saved images :\", num_saved)\nprint(\"Failed records:\", num_failed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T17:18:28.521648Z","iopub.execute_input":"2025-11-16T17:18:28.522143Z","iopub.status.idle":"2025-11-16T17:18:54.588616Z","shell.execute_reply.started":"2025-11-16T17:18:28.522117Z","shell.execute_reply":"2025-11-16T17:18:54.587594Z"}},"outputs":[{"name":"stdout","text":"OUT_CLASS_DIR reset: /kaggle/working/train_images/all\nFound shards: 60\nProcessing /kaggle/input/test-gan-competition/shard_00/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_48/2165087500.py:81: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n  img = Image.fromarray(arr8, mode=\"L\")\n","output_type":"stream"},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_01/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_02/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_03/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_04/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_05/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_06/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_07/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_08/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_09/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_10/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_11/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_12/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_13/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_14/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_15/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_16/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_17/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_18/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_19/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_20/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_21/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_22/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_23/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_24/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4374215d0264d2991a5cd74e1e68b78"}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_25/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd5f6476d63e4f8bad9c054dbf1b25d3"}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_26/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ff85840115843b99543716870941bef"}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_27/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94328de995944f9a85a696768b8d6762"}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_28/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ed79b017ea64a29b41c8f56d49bc2a0"}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_29/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47dd466259ca41608bb1c8dea0464e1f"}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_30/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aed567453f0b4cd29f4c5bb9d781359b"}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_31/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d40dedd438964e6380f0f9ca9b8aa257"}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_32/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba7b0df611c94a6eaf31848d8ca346f9"}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_33/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8d17a0a19674ff08833eabb89e6d216"}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_34/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0153d054264a4e5884650c294c46f669"}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_35/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"419a32c202854d3ab36da5eeadfe545f"}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_36/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da5127b0d0874be4b6af0fff2c2dc194"}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_37/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4c72bfb7bce4125ac28652d314c299b"}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_38/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4add2d2c012c4e2abf55dec4bcad5fc3"}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_39/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"571d248a9b5f41d8a066dbfac1b8e9ff"}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_40/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32144e51b8a34b9ebee2b29359a61bfd"}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_41/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1d42871a5964071bea5d14a6be20c76"}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_42/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"497d8d3bfab74cbeb02c7d91358c3a18"}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_43/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4720bac02011455aacac4274ed72f09d"}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_44/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02e60a625e4245a88f8d6698b0f01f8a"}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_45/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8688c83f8aeb4bbeb476107c41cd170d"}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_46/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"966a6ff350664cb3934dc7537e4472a6"}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_47/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e6bf598571c4c5b8527bf906225b9b9"}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_48/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1b4cf73e6974e0ca0e7485a34c1ad59"}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_49/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ff53c9748d44c439feec9b267c81f02"}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_50/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cefd8d309286421d94efef40df2df752"}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_51/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f18946306dab4e0aa506cccc8f96aa37"}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_52/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f7611542953447ab3ab3d591067ff67"}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_53/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0a66f47d8af46eb814f4f0a212e607c"}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_54/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5af4d531775b4d7da08c0b9643d640a4"}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_55/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd2320975a684053a8749a5a2442b963"}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_56/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3f539a46ab1461ca733c5ce1722594b"}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_57/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8a5f3f2072042c79d2d3875bf9188a2"}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_58/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12b060bf31d44101b3d629432b0b21a8"}},"metadata":{}},{"name":"stdout","text":"Processing /kaggle/input/test-gan-competition/shard_59/data.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01c8b99937f9428ab0df2f4c7d508dc9"}},"metadata":{}},{"name":"stdout","text":"Saved images : 58852\nFailed records: 1148\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from glob import glob\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nsample_paths = sorted(glob(os.path.join(OUT_CLASS_DIR, \"*.png\")))[:16]\nplt.figure(figsize=(6, 6))\nfor i, p in enumerate(sample_paths):\n    img = Image.open(p)\n    plt.subplot(4, 4, i + 1)\n    plt.imshow(img, cmap=\"gray\")\n    plt.axis(\"off\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T17:19:09.309343Z","iopub.execute_input":"2025-11-16T17:19:09.309936Z","iopub.status.idle":"2025-11-16T17:19:09.783686Z","shell.execute_reply.started":"2025-11-16T17:19:09.309907Z","shell.execute_reply":"2025-11-16T17:19:09.782910Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 600x600 with 16 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom glob import glob\nfrom PIL import Image\nfrom tqdm.auto import tqdm\n\n# adjust if your path is different\nOUT_ROOT = \"/kaggle/working/train_images\"\nOUT_CLASS_DIR = os.path.join(OUT_ROOT, \"all\")\n\nIMG_DIR = OUT_CLASS_DIR\npaths = sorted(glob(os.path.join(IMG_DIR, \"*.png\")))\nprint(\"Total decoded PNGs found:\", len(paths))\n\nrows = []\nfor p in tqdm(paths):\n    img = Image.open(p).convert(\"L\")   # grayscale for stats\n    arr = np.array(img, dtype=np.float32) / 255.0\n\n    mean = float(arr.mean())\n    std  = float(arr.std())\n    min_v = float(arr.min())\n    max_v = float(arr.max())\n    frac_near0 = float((arr < 0.02).mean())   # fraction very dark\n    frac_near1 = float((arr > 0.98).mean())   # fraction very bright\n\n    rows.append({\n        \"path\": p,\n        \"mean\": mean,\n        \"std\": std,\n        \"min\": min_v,\n        \"max\": max_v,\n        \"frac_near0\": frac_near0,\n        \"frac_near1\": frac_near1,\n    })\n\nstats_df = pd.DataFrame(rows)\nstats_df.to_csv(\"image_stats.csv\", index=False)\nprint(\"Saved stats to image_stats.csv\")\nstats_df.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T17:19:12.497834Z","iopub.execute_input":"2025-11-16T17:19:12.498111Z","iopub.status.idle":"2025-11-16T17:19:27.859260Z","shell.execute_reply.started":"2025-11-16T17:19:12.498092Z","shell.execute_reply":"2025-11-16T17:19:27.858629Z"}},"outputs":[{"name":"stdout","text":"Total decoded PNGs found: 58852\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/58852 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ee42e7f1c1849b88d6bdb2a2c2757be"}},"metadata":{}},{"name":"stdout","text":"Saved stats to image_stats.csv\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"               mean           std           min      max    frac_near0  \\\ncount  58852.000000  58852.000000  58852.000000  58852.0  58852.000000   \nmean       0.421228      0.353278      0.027286      1.0      0.571684   \nstd        0.325246      0.108727      0.113239      0.0      0.331394   \nmin        0.027344      0.000000      0.000000      1.0      0.000000   \n25%        0.104492      0.285958      0.000000      1.0      0.261719   \n50%        0.288086      0.405178      0.000000      1.0      0.710938   \n75%        0.738281      0.439570      0.000000      1.0      0.895508   \nmax        1.000000      0.499355      1.000000      1.0      0.972656   \n\n         frac_near1  \ncount  58852.000000  \nmean       0.416954  \nstd        0.320583  \nmin        0.027344  \n25%        0.104492  \n50%        0.280273  \n75%        0.738281  \nmax        1.000000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>max</th>\n      <th>frac_near0</th>\n      <th>frac_near1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>58852.000000</td>\n      <td>58852.000000</td>\n      <td>58852.000000</td>\n      <td>58852.0</td>\n      <td>58852.000000</td>\n      <td>58852.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.421228</td>\n      <td>0.353278</td>\n      <td>0.027286</td>\n      <td>1.0</td>\n      <td>0.571684</td>\n      <td>0.416954</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.325246</td>\n      <td>0.108727</td>\n      <td>0.113239</td>\n      <td>0.0</td>\n      <td>0.331394</td>\n      <td>0.320583</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.027344</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n      <td>0.000000</td>\n      <td>0.027344</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.104492</td>\n      <td>0.285958</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n      <td>0.261719</td>\n      <td>0.104492</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.288086</td>\n      <td>0.405178</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n      <td>0.710938</td>\n      <td>0.280273</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.738281</td>\n      <td>0.439570</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n      <td>0.895508</td>\n      <td>0.738281</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n      <td>0.499355</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n      <td>0.972656</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"## Define a first-pass “bad image” rule","metadata":{}},{"cell_type":"code","source":"# Heuristic filters for almost-blank / almost-constant images\nblankish = (\n    (stats_df[\"std\"] < 0.02) |\n    (stats_df[\"mean\"] < 0.01) |\n    (stats_df[\"mean\"] > 0.99) |\n    (stats_df[\"frac_near0\"] > 0.98) |\n    (stats_df[\"frac_near1\"] > 0.98)\n)\n\nprint(\"Blank-ish images flagged:\", blankish.sum(), \"out of\", len(stats_df))\n\nclean_df = stats_df[~blankish].reset_index(drop=True)\nprint(\"Remaining (clean) images:\", len(clean_df))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T17:19:34.347903Z","iopub.execute_input":"2025-11-16T17:19:34.348393Z","iopub.status.idle":"2025-11-16T17:19:34.365122Z","shell.execute_reply.started":"2025-11-16T17:19:34.348370Z","shell.execute_reply":"2025-11-16T17:19:34.364389Z"}},"outputs":[{"name":"stdout","text":"Blank-ish images flagged: 756 out of 58852\nRemaining (clean) images: 58096\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Copy filtered images into a clean dataset folder","metadata":{}},{"cell_type":"code","source":"import shutil\n\nCLEAN_ROOT = \"/kaggle/working/train_images_clean\"\nCLEAN_DIR  = os.path.join(CLEAN_ROOT, \"all\")\n\n# reset clean directory\nif os.path.exists(CLEAN_ROOT):\n    shutil.rmtree(CLEAN_ROOT)\n\nos.makedirs(CLEAN_DIR, exist_ok=True)\n\nfor p in tqdm(clean_df[\"path\"], desc=\"Copying clean images\"):\n    fname = os.path.basename(p)\n    dst = os.path.join(CLEAN_DIR, fname)\n    shutil.copy2(p, dst)\n\nprint(\"Copied\", len(clean_df), \"images to\", CLEAN_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T17:19:41.453891Z","iopub.execute_input":"2025-11-16T17:19:41.454570Z","iopub.status.idle":"2025-11-16T17:19:47.426518Z","shell.execute_reply.started":"2025-11-16T17:19:41.454544Z","shell.execute_reply":"2025-11-16T17:19:47.425593Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Copying clean images:   0%|          | 0/58096 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c1607f789174af6b92f52334a49b23d"}},"metadata":{}},{"name":"stdout","text":"Copied 58096 images to /kaggle/working/train_images_clean/all\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Pack cleaned images into a .npy array for fast loading","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom glob import glob\nfrom PIL import Image\nfrom tqdm.auto import tqdm\n\nCLEAN_ROOT = \"/kaggle/working/train_images_clean\"\nCLEAN_DIR  = os.path.join(CLEAN_ROOT, \"all\")\n\npaths = sorted(glob(os.path.join(CLEAN_DIR, \"*.png\")))\nprint(\"Clean PNGs found:\", len(paths))\n\n# We'll store as uint8 [0..255], CHW format\nN = len(paths)\narr = np.empty((N, 3, 32, 32), dtype=np.uint8)\n\nfor i, p in enumerate(tqdm(paths)):\n    img = Image.open(p).convert(\"RGB\")\n    img = img.resize((32, 32), Image.BICUBIC)  # should already be 32x32, but safe\n    x = np.array(img, dtype=np.uint8)          # H,W,3\n    x = np.transpose(x, (2, 0, 1))             # 3,H,W\n    arr[i] = x\n\nnp.save(\"clean_images_uint8.npy\", arr)\nprint(\"Saved clean_images_uint8.npy with shape:\", arr.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T17:19:54.761178Z","iopub.execute_input":"2025-11-16T17:19:54.761887Z","iopub.status.idle":"2025-11-16T17:20:01.948614Z","shell.execute_reply.started":"2025-11-16T17:19:54.761865Z","shell.execute_reply":"2025-11-16T17:20:01.948009Z"}},"outputs":[{"name":"stdout","text":"Clean PNGs found: 58096\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/58096 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c891489b2e9f401a8cb59dbb5f21bd6a"}},"metadata":{}},{"name":"stdout","text":"Saved clean_images_uint8.npy with shape: (58096, 3, 32, 32)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## DataLoader on the cleaned-Fast Dataset with [-1, 1] normalization","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass NpyImageDataset(Dataset):\n    def __init__(self, npy_path):\n        # memmap for speed & low RAM\n        self.data = np.load(npy_path, mmap_mode=\"r\")  # shape (N,3,32,32), uint8\n\n    def __len__(self):\n        return self.data.shape[0]\n\n    def __getitem__(self, idx):\n        x = self.data[idx].astype(np.float32)  # (3,32,32) in [0..255]\n        x = x / 127.5 - 1.0                    # -> [-1,1]\n        x = torch.from_numpy(x)               # torch.float32\n        y = 0                                  # dummy label (unused for GAN)\n        return x, y\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\n\ndataset = NpyImageDataset(\"clean_images_uint8.npy\")\nloader = DataLoader(\n    dataset,\n    batch_size=128,\n    shuffle=True,\n    num_workers=2,\n    pin_memory=True,\n)\n\nprint(\"Dataset size:\", len(dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T17:20:25.568013Z","iopub.execute_input":"2025-11-16T17:20:25.568710Z","iopub.status.idle":"2025-11-16T17:20:31.763688Z","shell.execute_reply.started":"2025-11-16T17:20:25.568684Z","shell.execute_reply":"2025-11-16T17:20:31.763052Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nDataset size: 58096\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## Model- Generator + Discriminator with no normalization","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\nnz  = 128\nngf = 64\nndf = 64\nnc  = 3\n\nclass Generator(nn.Module):\n    def __init__(self, nz=nz, ngf=ngf, nc=nc):\n        super().__init__()\n        self.net = nn.Sequential(\n            # input: (nz,1,1)\n            nn.ConvTranspose2d(nz, ngf * 4, 4, 1, 0, bias=False),  # 4x4\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),  # 8x8\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),      # 16x16\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),           # 32x32\n            nn.Tanh(),  # outputs in [-1,1]\n        )\n\n    def forward(self, z):\n        return self.net(z)\n\n\nclass Critic(nn.Module):\n    \"\"\"\n    WGAN-GP critic with:\n      - Conv layers\n      - NO normalization (no BatchNorm, no InstanceNorm)\n    \"\"\"\n    def __init__(self, ndf=ndf, nc=nc):\n        super().__init__()\n        self.net = nn.Sequential(\n            # input: (nc, 32, 32)\n            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),       # 16x16\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),  # 8x8\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),  # 4x4\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.Conv2d(ndf * 4, 1, 4, 1, 0, bias=False),    # 1x1\n        )\n\n    def forward(self, x):\n        out = self.net(x)\n        return out.view(-1)   # (batch,)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T17:20:39.630870Z","iopub.execute_input":"2025-11-16T17:20:39.631559Z","iopub.status.idle":"2025-11-16T17:20:39.639095Z","shell.execute_reply.started":"2025-11-16T17:20:39.631534Z","shell.execute_reply":"2025-11-16T17:20:39.638532Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"G = Generator(nz=nz).to(device)\nD = Critic().to(device)\n\nprint(G)\nprint(D)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T17:20:42.606414Z","iopub.execute_input":"2025-11-16T17:20:42.607340Z","iopub.status.idle":"2025-11-16T17:20:42.931135Z","shell.execute_reply.started":"2025-11-16T17:20:42.607294Z","shell.execute_reply":"2025-11-16T17:20:42.930303Z"}},"outputs":[{"name":"stdout","text":"Generator(\n  (net): Sequential(\n    (0): ConvTranspose2d(128, 256, kernel_size=(4, 4), stride=(1, 1), bias=False)\n    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU(inplace=True)\n    (6): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (8): ReLU(inplace=True)\n    (9): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (10): Tanh()\n  )\n)\nCritic(\n  (net): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n    (4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n    (6): Conv2d(256, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n  )\n)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"def gradient_penalty(D, real, fake, device, lambda_gp=10.0):\n    bsz = real.size(0)\n    alpha = torch.rand(bsz, 1, 1, 1, device=device)\n    interp = alpha * real + (1 - alpha) * fake\n    interp.requires_grad_(True)\n\n    d_interp = D(interp)\n    grads = torch.autograd.grad(\n        outputs=d_interp,\n        inputs=interp,\n        grad_outputs=torch.ones_like(d_interp),\n        create_graph=True,\n        retain_graph=True,\n        only_inputs=True,\n    )[0]\n\n    grads = grads.view(bsz, -1)\n    gp = ((grads.norm(2, dim=1) - 1.0) ** 2).mean()\n    return lambda_gp * gp","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T17:20:47.407467Z","iopub.execute_input":"2025-11-16T17:20:47.408006Z","iopub.status.idle":"2025-11-16T17:20:47.413301Z","shell.execute_reply.started":"2025-11-16T17:20:47.407982Z","shell.execute_reply":"2025-11-16T17:20:47.412530Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## WGAN-GP training loop","metadata":{}},{"cell_type":"code","source":"from torchvision.utils import save_image\nimport os\n\nlr = 1e-4\nlambda_gp = 5.0   # you can keep this if it worked well\nn_critic = 5\nnum_epochs = 20\n\noptD = torch.optim.Adam(D.parameters(), lr=lr, betas=(0.0, 0.9))\noptG = torch.optim.Adam(G.parameters(), lr=lr, betas=(0.0, 0.9))\n\nfixed_noise = torch.randn(64, nz, 1, 1, device=device)\n\nos.makedirs(\"samples_clean\", exist_ok=True)\nos.makedirs(\"checkpoints_clean\", exist_ok=True)\n\nfor epoch in range(1, num_epochs + 1):\n    G.train()\n    D.train()\n    running_lossD = 0.0\n    running_lossG = 0.0\n    n_batches = 0\n\n    for real, _ in loader:   # <- notice: loader from npy dataset\n        real = real.to(device)\n        bsz = real.size(0)\n        n_batches += 1\n\n        # ------ Critic update ------\n        for _ in range(n_critic):\n            z = torch.randn(bsz, nz, 1, 1, device=device)\n            fake = G(z).detach()\n\n            D_real = D(real)\n            D_fake = D(fake)\n\n            gp = gradient_penalty(D, real, fake, device, lambda_gp=lambda_gp)\n            lossD = -(D_real.mean() - D_fake.mean()) + gp\n\n            optD.zero_grad()\n            lossD.backward()\n            optD.step()\n\n        # ------ Generator update ------\n        z = torch.randn(bsz, nz, 1, 1, device=device)\n        fake = G(z)\n        D_fake = D(fake)\n        lossG = -D_fake.mean()\n\n        optG.zero_grad()\n        lossG.backward()\n        optG.step()\n\n        running_lossD += lossD.item()\n        running_lossG += lossG.item()\n\n    avg_lossD = running_lossD / n_batches\n    avg_lossG = running_lossG / n_batches\n\n    # sample images\n    G.eval()\n    with torch.no_grad():\n        samples = G(fixed_noise).cpu()\n        samples = (samples + 1) / 2  # [-1,1] -> [0,1]\n        save_image(samples, f\"samples_clean/epoch_{epoch:03d}.png\", nrow=8)\n\n    # save checkpoint\n    torch.save(G.state_dict(), f\"checkpoints_clean/G_epoch_{epoch:03d}.pth\")\n    torch.save(D.state_dict(), f\"checkpoints_clean/D_epoch_{epoch:03d}.pth\")\n\n    print(f\"Epoch {epoch}/{num_epochs} | D: {avg_lossD:.3f} | G: {avg_lossG:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T17:25:57.483547Z","iopub.execute_input":"2025-11-16T17:25:57.484302Z","iopub.status.idle":"2025-11-16T17:59:39.884570Z","shell.execute_reply.started":"2025-11-16T17:25:57.484267Z","shell.execute_reply":"2025-11-16T17:59:39.883722Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20 | D: -49.128 | G: -48.663\nEpoch 2/20 | D: -25.076 | G: -42.363\nEpoch 3/20 | D: -20.326 | G: -34.263\nEpoch 4/20 | D: -17.652 | G: -30.597\nEpoch 5/20 | D: -15.998 | G: -28.092\nEpoch 6/20 | D: -14.747 | G: -26.217\nEpoch 7/20 | D: -13.583 | G: -24.540\nEpoch 8/20 | D: -12.880 | G: -22.668\nEpoch 9/20 | D: -12.391 | G: -21.258\nEpoch 10/20 | D: -12.123 | G: -19.740\nEpoch 11/20 | D: -11.988 | G: -18.116\nEpoch 12/20 | D: -11.608 | G: -17.414\nEpoch 13/20 | D: -11.481 | G: -16.680\nEpoch 14/20 | D: -11.081 | G: -15.806\nEpoch 15/20 | D: -11.121 | G: -15.322\nEpoch 16/20 | D: -10.892 | G: -14.611\nEpoch 17/20 | D: -10.809 | G: -13.782\nEpoch 18/20 | D: -10.740 | G: -13.229\nEpoch 19/20 | D: -10.744 | G: -12.834\nEpoch 20/20 | D: -10.606 | G: -12.036\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import time\nimport torch\nn_critic=5\ndef estimate_epoch_time(num_test_batches=10, lambda_gp=5.0):\n    \"\"\"\n    Estimate how long ONE epoch will take for the current WGAN-GP setup.\n\n    - Uses your existing `loader`, `G`, `D`, `nz`, `n_critic`, `gradient_penalty`, `device`.\n    - Runs forward + backward like real training, but does NOT call optimizer.step(),\n      so weights are not updated.\n    - num_test_batches: how many batches to time; more = more accurate, slower.\n    \"\"\"\n    G.train()\n    D.train()\n\n    batches_timed = 0\n    start = None\n\n    for real, _ in loader:\n        real = real.to(device)\n        bsz = real.size(0)\n\n        # start timing after first batch to skip any initial overhead\n        if start is None:\n            if device == \"cuda\":\n                torch.cuda.synchronize()\n            start = time.time()\n\n        # ---- mimic critic work n_critic times ----\n        for _ in range(n_critic):\n            z = torch.randn(bsz, nz, 1, 1, device=device)\n            fake = G(z).detach()\n\n            D_real = D(real)\n            D_fake = D(fake)\n\n            gp = gradient_penalty(D, real, fake, device, lambda_gp=lambda_gp)\n            lossD = -(D_real.mean() - D_fake.mean()) + gp\n\n            D.zero_grad()\n            G.zero_grad()\n            lossD.backward()   # no optD.step()\n\n        # ---- mimic generator work ----\n        z = torch.randn(bsz, nz, 1, 1, device=device)\n        fake = G(z)\n        D_fake = D(fake)\n        lossG = -D_fake.mean()\n\n        D.zero_grad()\n        G.zero_grad()\n        lossG.backward()       # no optG.step()\n\n        batches_timed += 1\n        if batches_timed >= num_test_batches:\n            break\n\n    if device == \"cuda\":\n        torch.cuda.synchronize()\n    elapsed = time.time() - start\n\n    avg_batch_time = elapsed / batches_timed\n    est_epoch_time = avg_batch_time * len(loader)\n\n    print(f\"Timed {batches_timed} batches\")\n    print(f\"Average time per batch: {avg_batch_time:.3f} sec\")\n    print(f\"Approximate time per epoch: {est_epoch_time:.2f} sec \"\n          f\"(~{est_epoch_time/60:.2f} min)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T17:21:09.014803Z","iopub.execute_input":"2025-11-16T17:21:09.015426Z","iopub.status.idle":"2025-11-16T17:21:09.023398Z","shell.execute_reply.started":"2025-11-16T17:21:09.015401Z","shell.execute_reply":"2025-11-16T17:21:09.022818Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"estimate_epoch_time(num_test_batches=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T17:21:11.766849Z","iopub.execute_input":"2025-11-16T17:21:11.767754Z","iopub.status.idle":"2025-11-16T17:21:16.463874Z","shell.execute_reply.started":"2025-11-16T17:21:11.767720Z","shell.execute_reply":"2025-11-16T17:21:16.463086Z"}},"outputs":[{"name":"stdout","text":"Timed 10 batches\nAverage time per batch: 0.463 sec\nApproximate time per epoch: 210.35 sec (~3.51 min)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"from torchvision.utils import save_image\n\ndef generate_1000_images(G, nz, device, out_dir=\"generated_1000\"):\n    \"\"\"\n    Generate 1000 images with a trained Generator `G`.\n\n    - Assumes G maps noise z ~ N(0,1) of shape (nz,1,1) to images in [-1,1].\n    - Saves PNGs named: dig-000000.png ... dig-000999.png in `out_dir`.\n    \"\"\"\n    os.makedirs(out_dir, exist_ok=True)\n\n    G.eval()\n    num_samples = 1000\n    batch_size = 64\n\n    with torch.no_grad():\n        idx = 0\n        while idx < num_samples:\n            cur_batch = min(batch_size, num_samples - idx)\n            # sample latent codes\n            z = torch.randn(cur_batch, nz, 1, 1, device=device)\n            fake = G(z).cpu()              # shape: (B, 3, 32, 32), range [-1,1]\n            fake = (fake + 1) / 2.0        # -> [0,1] for saving\n\n            for j in range(cur_batch):\n                img = fake[j]\n                img_id = f\"dig-{(idx + j):06d}\"\n                out_path = os.path.join(out_dir, f\"{img_id}.png\")\n                save_image(img, out_path)\n\n            idx += cur_batch\n\n    print(f\"Generated {num_samples} images in: {out_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T18:07:13.451777Z","iopub.execute_input":"2025-11-16T18:07:13.452556Z","iopub.status.idle":"2025-11-16T18:07:13.458167Z","shell.execute_reply.started":"2025-11-16T18:07:13.452528Z","shell.execute_reply":"2025-11-16T18:07:13.457393Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# load trained generator for epoch 20\n\nG = Generator(nz=nz).to(device)\nG.load_state_dict(torch.load(\"checkpoints_clean/G_epoch_020.pth\", map_location=device))\n\ngenerate_1000_images(G, nz=nz, device=device, out_dir=\"generated_clean_epoch_020\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T18:07:59.359282Z","iopub.execute_input":"2025-11-16T18:07:59.360007Z","iopub.status.idle":"2025-11-16T18:07:59.923766Z","shell.execute_reply.started":"2025-11-16T18:07:59.359977Z","shell.execute_reply":"2025-11-16T18:07:59.923110Z"}},"outputs":[{"name":"stdout","text":"Generated 1000 images in: generated_clean_epoch_020\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# Sanity checking before submission\n\nimport os\nfrom glob import glob\n\nimport numpy as np\nfrom PIL import Image\n\nimport torch\nfrom torchvision.models import inception_v3, Inception_V3_Weights\nfrom torchvision import transforms\n\nfrom scipy.linalg import sqrtm  # if ImportError -> run: !pip install scipy\n\n\n# --- adjust these if your paths are different ---\nREAL_DIR = CLEAN_DIR     # e.g. \"/kaggle/working/train_images/all\"\nGEN_DIR  = \"/kaggle/working/generated_clean_epoch_020\"           # e.g. \"/kaggle/working/generated\"\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\n\n# ---------------- Inception model & preprocessing ----------------\ninception = inception_v3(\n    weights=Inception_V3_Weights.IMAGENET1K_V1,\n    aux_logits=True\n)\ninception.fc = torch.nn.Identity()\ninception.to(device)\ninception.eval()\n\npreproc = transforms.Compose([\n    transforms.Resize((299, 299)),\n    transforms.ToTensor(),   # NOTE: no extra normalization (as per competition)\n])\n\n@torch.no_grad()\ndef extract_feature_array(paths):\n    feats = []\n    for p in paths:\n        img = Image.open(p).convert(\"RGB\")\n        x = preproc(img).unsqueeze(0).to(device)\n        f = inception(x)\n        if isinstance(f, (list, tuple)):\n            f = f[0]\n        feats.append(f.squeeze().cpu().numpy())\n    return np.stack(feats, axis=0)  # (N, 2048)\n\ndef calculate_fid(mu1, sigma1, mu2, sigma2, eps=1e-6):\n    \"\"\"Standard FID using scipy.linalg.sqrtm.\"\"\"\n    diff = mu1 - mu2\n    cov_prod = sigma1.dot(sigma2)\n\n    covmean = sqrtm(cov_prod)\n    if not np.isfinite(covmean).all():\n        covmean = covmean.real\n        covmean += np.eye(covmean.shape[0]) * eps\n        covmean = sqrtm(covmean)\n\n    if np.iscomplexobj(covmean):\n        covmean = covmean.real\n\n    fid = diff.dot(diff) + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n    return float(fid)\n\n\n# ---------------- Train vs Train FID ----------------\nreal_paths_all = sorted(glob(os.path.join(REAL_DIR, \"*.png\")))\nprint(\"Total real PNGs:\", len(real_paths_all))\n\nsubset_size = min(1000, len(real_paths_all) // 2)\npaths_A = real_paths_all[:subset_size]\npaths_B = real_paths_all[subset_size:subset_size + subset_size]\n\nprint(f\"Train vs Train: using {len(paths_A)} + {len(paths_B)} images\")\n\nfeats_A = extract_feature_array(paths_A)\nfeats_B = extract_feature_array(paths_B)\n\nmu_A, sigma_A = feats_A.mean(axis=0), np.cov(feats_A, rowvar=False)\nmu_B, sigma_B = feats_B.mean(axis=0), np.cov(feats_B, rowvar=False)\n\nfid_train_vs_train = calculate_fid(mu_A, sigma_A, mu_B, sigma_B)\nprint(\"Local FID (train vs train):\", fid_train_vs_train)\n\n\n# ---------------- Offline FID (Real vs Generated) ----------------\ngen_paths_all = sorted(glob(os.path.join(GEN_DIR, \"dig-*.png\")))\nprint(\"Generated PNGs:\", len(gen_paths_all))\n\nn = min(1000, len(real_paths_all), len(gen_paths_all))\nreal_paths_for_fid = real_paths_all[:n]\ngen_paths_for_fid  = gen_paths_all[:n]\n\nprint(f\"Real vs Generated: using {n} real and {n} generated images\")\n\nfeats_real = extract_feature_array(real_paths_for_fid)\nfeats_fake = extract_feature_array(gen_paths_for_fid)\n\nmu_real, sigma_real = feats_real.mean(axis=0), np.cov(feats_real, rowvar=False)\nmu_fake, sigma_fake = feats_fake.mean(axis=0), np.cov(feats_fake, rowvar=False)\n\nfid_real_fake = calculate_fid(mu_real, sigma_real, mu_fake, sigma_fake)\nprint(\"Offline FID (real vs generated):\", fid_real_fake)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T18:12:16.806300Z","iopub.execute_input":"2025-11-16T18:12:16.807299Z","iopub.status.idle":"2025-11-16T18:13:30.291497Z","shell.execute_reply.started":"2025-11-16T18:12:16.807271Z","shell.execute_reply":"2025-11-16T18:13:30.290646Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nTotal real PNGs: 58096\nTrain vs Train: using 1000 + 1000 images\nLocal FID (train vs train): 5.073100595314411\nGenerated PNGs: 1000\nReal vs Generated: using 1000 real and 1000 generated images\nOffline FID (real vs generated): 28.32088170319708\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"## 7. Inception-V3 feature extraction","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom PIL import Image\n\nimport torch\nfrom torchvision.models import inception_v3, Inception_V3_Weights\nfrom torchvision import transforms\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nm = inception_v3(weights=Inception_V3_Weights.IMAGENET1K_V1, aux_logits=True)\nm.fc = torch.nn.Identity()\nm.to(device)\nm.eval()\n\npreproc = transforms.Compose([\n    transforms.Resize((299, 299)),\n    transforms.ToTensor(),\n    # Note: challenge code snippet didn't normalize; follow that exactly\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T18:13:41.323693Z","iopub.execute_input":"2025-11-16T18:13:41.324260Z","iopub.status.idle":"2025-11-16T18:13:41.890544Z","shell.execute_reply.started":"2025-11-16T18:13:41.324238Z","shell.execute_reply":"2025-11-16T18:13:41.889774Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def extract_feature(img_path):\n    img = Image.open(img_path).convert(\"RGB\")\n    x = preproc(img).unsqueeze(0).to(device)\n    with torch.no_grad():\n        feat = m(x)\n        if isinstance(feat, (list, tuple)):\n            feat = feat[0]\n    return feat.squeeze().cpu().numpy()  # (2048,)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T18:13:56.463596Z","iopub.execute_input":"2025-11-16T18:13:56.464198Z","iopub.status.idle":"2025-11-16T18:13:56.468020Z","shell.execute_reply.started":"2025-11-16T18:13:56.464174Z","shell.execute_reply":"2025-11-16T18:13:56.467459Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"ids = [f\"dig-{i:06d}\" for i in range(1000)]\nfeatures = []\n\nfor img_id in ids:\n    img_path = os.path.join(\"generated_clean_epoch_020\", f\"{img_id}.png\")\n    feats = extract_feature(img_path)\n    features.append(feats)\n\nfeatures = np.stack(features, axis=0)  # (1000, 2048)\n\ndf = pd.DataFrame(features, columns=[f\"f{i}\" for i in range(2048)])\ndf.insert(0, \"id\", ids)\n\ndf.to_csv(\"submission.csv\", index=False)\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T18:14:10.132217Z","iopub.execute_input":"2025-11-16T18:14:10.132946Z","iopub.status.idle":"2025-11-16T18:14:26.274141Z","shell.execute_reply.started":"2025-11-16T18:14:10.132918Z","shell.execute_reply":"2025-11-16T18:14:26.273479Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"           id        f0        f1        f2        f3        f4        f5  \\\n0  dig-000000  0.078271  0.022539  0.017287  0.010455  0.011898  0.003824   \n1  dig-000001  0.291619  0.024258  0.012572  0.123117  0.026477  0.055493   \n2  dig-000002  0.126000  0.067164  0.051506  0.052585  0.040942  0.007126   \n3  dig-000003  0.199603  0.042408  0.025110  0.056331  0.060045  0.148724   \n4  dig-000004  0.140865  0.040671  0.050942  0.013002  0.026795  0.032774   \n\n         f6        f7        f8  ...     f2038     f2039     f2040     f2041  \\\n0  0.114708  0.065960  0.030890  ...  0.231531  0.137628  0.112003  1.678905   \n1  0.842750  0.262093  0.204759  ...  0.095390  0.757889  0.082185  0.717655   \n2  0.194295  0.085232  0.081916  ...  0.151672  0.613133  0.202065  1.485876   \n3  0.806377  0.233671  0.284124  ...  0.031978  0.307023  0.157011  0.634420   \n4  0.051073  0.068163  0.025142  ...  0.213705  0.097804  0.091468  1.825979   \n\n      f2042     f2043     f2044     f2045     f2046     f2047  \n0  0.263563  0.021727  0.026733  0.887074  0.000000  0.231381  \n1  0.142208  0.056776  0.508722  0.782439  0.086304  0.295154  \n2  0.301500  0.031909  0.031422  0.774008  0.011379  0.239280  \n3  0.143387  0.282817  0.505417  0.407519  0.090917  0.422052  \n4  0.230160  0.020341  0.005941  0.962366  0.000000  0.350124  \n\n[5 rows x 2049 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>f0</th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n      <th>f4</th>\n      <th>f5</th>\n      <th>f6</th>\n      <th>f7</th>\n      <th>f8</th>\n      <th>...</th>\n      <th>f2038</th>\n      <th>f2039</th>\n      <th>f2040</th>\n      <th>f2041</th>\n      <th>f2042</th>\n      <th>f2043</th>\n      <th>f2044</th>\n      <th>f2045</th>\n      <th>f2046</th>\n      <th>f2047</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>dig-000000</td>\n      <td>0.078271</td>\n      <td>0.022539</td>\n      <td>0.017287</td>\n      <td>0.010455</td>\n      <td>0.011898</td>\n      <td>0.003824</td>\n      <td>0.114708</td>\n      <td>0.065960</td>\n      <td>0.030890</td>\n      <td>...</td>\n      <td>0.231531</td>\n      <td>0.137628</td>\n      <td>0.112003</td>\n      <td>1.678905</td>\n      <td>0.263563</td>\n      <td>0.021727</td>\n      <td>0.026733</td>\n      <td>0.887074</td>\n      <td>0.000000</td>\n      <td>0.231381</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>dig-000001</td>\n      <td>0.291619</td>\n      <td>0.024258</td>\n      <td>0.012572</td>\n      <td>0.123117</td>\n      <td>0.026477</td>\n      <td>0.055493</td>\n      <td>0.842750</td>\n      <td>0.262093</td>\n      <td>0.204759</td>\n      <td>...</td>\n      <td>0.095390</td>\n      <td>0.757889</td>\n      <td>0.082185</td>\n      <td>0.717655</td>\n      <td>0.142208</td>\n      <td>0.056776</td>\n      <td>0.508722</td>\n      <td>0.782439</td>\n      <td>0.086304</td>\n      <td>0.295154</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>dig-000002</td>\n      <td>0.126000</td>\n      <td>0.067164</td>\n      <td>0.051506</td>\n      <td>0.052585</td>\n      <td>0.040942</td>\n      <td>0.007126</td>\n      <td>0.194295</td>\n      <td>0.085232</td>\n      <td>0.081916</td>\n      <td>...</td>\n      <td>0.151672</td>\n      <td>0.613133</td>\n      <td>0.202065</td>\n      <td>1.485876</td>\n      <td>0.301500</td>\n      <td>0.031909</td>\n      <td>0.031422</td>\n      <td>0.774008</td>\n      <td>0.011379</td>\n      <td>0.239280</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>dig-000003</td>\n      <td>0.199603</td>\n      <td>0.042408</td>\n      <td>0.025110</td>\n      <td>0.056331</td>\n      <td>0.060045</td>\n      <td>0.148724</td>\n      <td>0.806377</td>\n      <td>0.233671</td>\n      <td>0.284124</td>\n      <td>...</td>\n      <td>0.031978</td>\n      <td>0.307023</td>\n      <td>0.157011</td>\n      <td>0.634420</td>\n      <td>0.143387</td>\n      <td>0.282817</td>\n      <td>0.505417</td>\n      <td>0.407519</td>\n      <td>0.090917</td>\n      <td>0.422052</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>dig-000004</td>\n      <td>0.140865</td>\n      <td>0.040671</td>\n      <td>0.050942</td>\n      <td>0.013002</td>\n      <td>0.026795</td>\n      <td>0.032774</td>\n      <td>0.051073</td>\n      <td>0.068163</td>\n      <td>0.025142</td>\n      <td>...</td>\n      <td>0.213705</td>\n      <td>0.097804</td>\n      <td>0.091468</td>\n      <td>1.825979</td>\n      <td>0.230160</td>\n      <td>0.020341</td>\n      <td>0.005941</td>\n      <td>0.962366</td>\n      <td>0.000000</td>\n      <td>0.350124</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 2049 columns</p>\n</div>"},"metadata":{}}],"execution_count":27}]}